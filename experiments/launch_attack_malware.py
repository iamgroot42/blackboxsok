from simple_parsing import ArgumentParser
from pathlib import Path
import os
from bbeval.config import MalwareAttackerConfig, ExperimentConfig
from bbeval.datasets.utils import get_dataset_wrapper, get_target_label
from bbeval.datasets.base import CustomDatasetWrapper
from bbeval.attacker.utils import get_attack_wrapper
from bbeval.models.utils import get_model_wrapper
import torch as ch
from tqdm import tqdm


def get_model_and_aux_models(attacker_config: MalwareAttackerConfig):
    model_config = attacker_config.adv_model_config
    # Get first model
    target_model = get_model_wrapper(model_config)
    target_model.cuda()

    if attacker_config.aux_model_configs:
        aux_models = get_model_wrapper(attacker_config.aux_model_configs)
        for _key in aux_models:
            aux_models[_key].cuda()
    else:
        aux_models = {}
    return target_model, aux_models


def single_attack(target_model, aux_models, x_orig, x_sample_adv, y_label, y_target, attacker_config: MalwareAttackerConfig, experiment_config: ExperimentConfig):
    attacker = get_attack_wrapper(target_model, aux_models, attacker_config, experiment_config)
    x_sample_adv, queries_used = attacker.attack(x_orig, x_sample_adv, y_label, y_target)
    return (x_sample_adv, queries_used), attacker


# os.environ['TORCH_HOME'] = '/p/blackboxsok/models/imagenet_torch' # download imagenet models to project directory
if __name__ == "__main__":
    parser = ArgumentParser(add_help=False)
    parser.add_argument(
        "--config", help="Specify config file", type=Path)
    args = parser.parse_args()
    config = ExperimentConfig.load(args.config, drop_extra_fields=False)

    ds_config = config.dataset_config
    batch_size = config.batch_size

    # Get data-loader, make sure it works
    ds: CustomDatasetWrapper = get_dataset_wrapper(ds_config)
    _, _, test_loader = ds.get_loaders(
        batch_size=batch_size, eval_shuffle=False)

    # Extract configs
    attacker_config_1: MalwareAttackerConfig = config.first_attack_config()
    attacker_config_2: MalwareAttackerConfig = config.second_attack_config()

    # Load up model(s)
    target_model_1, aux_models_1 = get_model_and_aux_models(attacker_config_1)
    num_class = ds.num_classes

    # TODO: Fix logging of results

    num_total = 0
    num_orig = 0
    num_misclassified = 0
    counter = 1
    savedir = "/p/blackboxsok/malware_samples/"
    os.makedirs(os.path.join(savedir, config.experiment_name), exist_ok=True)
    for x_orig in tqdm(test_loader, desc="Generating adversarial examples"):
        y_label = target_model_1.predict_proba(x_orig)
        num_orig += ch.sum(ch.argmax(y_label, 1) == 0).item()

        # Perform attack
        total_queries_used = 0
        (x_sample_adv, queries_used_1), attacker_1 = single_attack(target_model_1,
                      aux_models=aux_models_1,
                      x_orig=x_orig,
                      x_sample_adv=x_orig,
                      y_label=y_label,
                      y_target=None,
                      attacker_config=attacker_config_1,
                      experiment_config=config)
        total_queries_used += queries_used_1
        # attacker_1.add_result(total_queries_used, )
        attacker_1.save_results()

        # Follow up attack if config provided
        if attacker_config_2:
            target_model_2, aux_models_2 = get_model_and_aux_models(attacker_config_2)
            (x_sample_adv, queries_used_2), attacker_2 = single_attack(target_model_2,
                                                                       aux_models=aux_models_2,
                                                                       x_orig=x_orig,
                                                                       x_sample_adv=x_sample_adv,
                                                                       y_label=y_label,
                                                                       y_target=None,
                                                                       attacker_config=attacker_config_2,
                                                                       experiment_config=config)
            total_queries_used += queries_used_2
            # attacker_2.add_result(total_queries_used)
            attacker_2.save_results()
            
        # Get label for perturbed sample
        y_label_adv = target_model_1.predict_proba(x_sample_adv)
        num_misclassified += ch.sum(ch.argmax(y_label_adv, 1) == 0).item()
        num_total += len(y_label_adv)
        
         # Save adversarial examples
        for x_ in x_sample_adv:
            with open(os.path.join(savedir, config.experiment_name, f"{counter}.exe"), "wb") as f:
                f.write(x_.bytes)
            counter += 1

    print("Baseline (clean data) evasion rate: %.3f" % (num_orig / num_total))
    print("Evasion rate: %.3f" % (num_misclassified / num_total))
