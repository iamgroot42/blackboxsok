"""
    Implementation borrowed from https://github.com/pralab/secml_malware
"""
import torch.nn as nn
import torch.functional as F
import torch as ch
# from secml.array import CArray
from bbeval.config import MalwareModelConfig
# from secml_malware.models.malconv import MalConv
# from secml_malware.models.c_classifier_end2end_malware import CClassifierEnd2EndMalware
from bbeval.models.pytorch.wrapper import PyTorchModelWrapper
# from secml_malware.models.c_classifier_end2end_malware import End2EndModel
from typing import List


class MLPClassifier(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        n_inp = 2381
        self.model = nn.Sequential(
            nn.Linear(n_inp, 2400),
            nn.ReLU(),
            nn.Linear(2400, 1200),
            nn.ReLU(),
            nn.Linear(1200, 1200),
            nn.ReLU(),
            nn.Linear(1200, 2)
        )


class SecmlMalConv(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        self.model = MalConv()
        self.model = CClassifierEnd2EndMalware(self.model)
        self.model.load_pretrained_model()
    
    def pre_process(self, X: List):
        return CArray([End2EndModel.bytes_to_numpy(
            x.bytes, self.model.get_input_max_length(), 256, False
        ) for x in X])
    
    def _forward(self, x):
        retobj = self.model.predict(x, True)
        preds = ch.from_numpy(retobj[1].tondarray()).cuda()
        return preds
    
    def cuda(self):
        # No concept of CUDA for this model
        return


# class MalConv(PyTorchModelWrapper):
#     def __init__(self, model_config: MalwareModelConfig):
#         super().__init__(model_config)
#         self.embedding_size = self.config.embedding_size
#         self.embedding_1 = nn.Embedding(
#             num_embeddings=257,
#             embedding_dim=self.embedding_size)
#         self.conv1d_1 = nn.Conv1d(in_channels=self.embedding_size,
#                                   out_channels=128, kernel_size=(500,),
#                                   stride=(500,), groups=1, bias=True)
#         self.conv1d_2 = nn.Conv1d(in_channels=self.embedding_size,
#                                   out_channels=128, kernel_size=(500,),
#                                   stride=(500,), groups=1, bias=True)
#         self.dense_1 = nn.Linear(in_features=128, out_features=128, bias=True)
#         self.dense_2 = nn.Linear(in_features=128, out_features=1, bias=True)

#     def pre_process_fn(self, x, transpose=True):
#         x_ = x.clone().detach().requires_grad_(True).type(ch.LongTensor)
#         x_ = x_.squeeze(dim=1)
#         emb_x = self.embedding_1(x_)
#         if transpose:
#             emb_x = ch.transpose(emb_x, 1, 2)
#         return emb_x

#     def forward(self, x, detach: bool = False):
#         conv1d_1 = self.conv1d_1(x)
#         conv1d_2 = self.conv1d_2(x)
#         conv1d_1_activation = ch.relu(conv1d_1)
#         conv1d_2_activation = ch.sigmoid(conv1d_2)
#         multiply_1 = conv1d_1_activation * conv1d_2_activation
#         global_max_pooling1d_1 = F.max_pool1d(
#             input=multiply_1, kernel_size=multiply_1.size()[2:])
#         global_max_pooling1d_1_flatten = global_max_pooling1d_1.view(
#             global_max_pooling1d_1.size(0), -1)
#         dense_1 = self.dense_1(global_max_pooling1d_1_flatten)
#         dense_1_activation = ch.relu(dense_1)
#         dense_2 = self.dense_2(dense_1_activation)
#         # dense_2_activation = ch.sigmoid(dense_2)
#         if detach:
#             dense_2 = dense_2.detach()
#         return dense_2
