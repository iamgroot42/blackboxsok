"""
    Implementation borrowed from https://github.com/pralab/secml_malware
"""
import torch.nn as nn
import torch.functional as F
import torch as ch
from bbeval.config import MalwareModelConfig
from bbeval.models.pytorch.wrapper import PyTorchModelWrapper


class MalConv(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        self.embedding_size = self.config.embedding_size
        self.embedding_1 = nn.Embedding(
            num_embeddings=257,
            embedding_dim=self.embedding_size)
        self.conv1d_1 = nn.Conv1d(in_channels=self.embedding_size,
            out_channels=128, kernel_size=(500,),
            stride=(500,), groups=1, bias=True)
        self.conv1d_2 = nn.Conv1d(in_channels=self.embedding_size,
        out_channels=128, kernel_size=(500,),
        stride=(500,), groups=1, bias=True)
        self.dense_1 = nn.Linear(in_features=128, out_features=128, bias=True)
        self.dense_2 = nn.Linear(in_features=128, out_features=1, bias=True)

    def pre_process_fn(self, x, transpose=True):
        x_ = x.clone().detach().requires_grad_(True).type(ch.LongTensor)
        x_ = x_.squeeze(dim=1)
        emb_x = self.embedding_1(x_)
        if transpose:
            emb_x = ch.transpose(emb_x, 1, 2)
        return emb_x

    def forward(self, x):
        conv1d_1 = self.conv1d_1(x)
        conv1d_2 = self.conv1d_2(x)
        conv1d_1_activation = ch.relu(conv1d_1)
        conv1d_2_activation = ch.sigmoid(conv1d_2)
        multiply_1 = conv1d_1_activation * conv1d_2_activation
        global_max_pooling1d_1 = F.max_pool1d(input=multiply_1, kernel_size=multiply_1.size()[2:])
        global_max_pooling1d_1_flatten = global_max_pooling1d_1.view(global_max_pooling1d_1.size(0), -1)
        dense_1 = self.dense_1(global_max_pooling1d_1_flatten)
        dense_1_activation = ch.relu(dense_1)
        dense_2 = self.dense_2(dense_1_activation)
        # dense_2_activation = ch.sigmoid(dense_2)
        return dense_2
