"""
    Implementation borrowed from https://github.com/pralab/secml_malware
"""
import torch.nn as nn
import torch.functional as F
import torch as ch
from bbeval.config import MalwareModelConfig
from secml_malware.models.malconv import MalConv
from secml_malware.models.c_classifier_end2end_malware import CClassifierEnd2EndMalware
from bbeval.models.pytorch.wrapper import PyTorchModelWrapper
from secml_malware.models.c_classifier_end2end_malware import End2EndModel


class MLPClassifier(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        n_inp = None  # TODO: Extract later
        self.model = nn.Sequential(
            nn.Linear(n_inp, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )


class SecmlMalConv(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        self.model = MalConv()
        self.model = CClassifierEnd2EndMalware(self.model)
        self.model.load_pretrained_model()

    def pre_process_fn(self, x):
        x_transformed = [End2EndModel.bytes_to_numpy(
            x_, self.model.get_input_max_length(), 256, False
            ) for x_ in x]
        return x_transformed
    
    def forward(self, x):
        return self.model(x)


class MalConv(PyTorchModelWrapper):
    def __init__(self, model_config: MalwareModelConfig):
        super().__init__(model_config)
        self.embedding_size = self.config.embedding_size
        self.embedding_1 = nn.Embedding(
            num_embeddings=257,
            embedding_dim=self.embedding_size)
        self.conv1d_1 = nn.Conv1d(in_channels=self.embedding_size,
                                  out_channels=128, kernel_size=(500,),
                                  stride=(500,), groups=1, bias=True)
        self.conv1d_2 = nn.Conv1d(in_channels=self.embedding_size,
                                  out_channels=128, kernel_size=(500,),
                                  stride=(500,), groups=1, bias=True)
        self.dense_1 = nn.Linear(in_features=128, out_features=128, bias=True)
        self.dense_2 = nn.Linear(in_features=128, out_features=1, bias=True)

    def pre_process_fn(self, x, transpose=True):
        x_ = x.clone().detach().requires_grad_(True).type(ch.LongTensor)
        x_ = x_.squeeze(dim=1)
        emb_x = self.embedding_1(x_)
        if transpose:
            emb_x = ch.transpose(emb_x, 1, 2)
        return emb_x

    def forward(self, x, detach: bool = False):
        conv1d_1 = self.conv1d_1(x)
        conv1d_2 = self.conv1d_2(x)
        conv1d_1_activation = ch.relu(conv1d_1)
        conv1d_2_activation = ch.sigmoid(conv1d_2)
        multiply_1 = conv1d_1_activation * conv1d_2_activation
        global_max_pooling1d_1 = F.max_pool1d(
            input=multiply_1, kernel_size=multiply_1.size()[2:])
        global_max_pooling1d_1_flatten = global_max_pooling1d_1.view(
            global_max_pooling1d_1.size(0), -1)
        dense_1 = self.dense_1(global_max_pooling1d_1_flatten)
        dense_1_activation = ch.relu(dense_1)
        dense_2 = self.dense_2(dense_1_activation)
        # dense_2_activation = ch.sigmoid(dense_2)
        if detach:
            dense_2 = dense_2.detach()
        return dense_2
